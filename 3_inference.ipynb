{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "import awkward as ak\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from source.dataset import JetClass, JetNet, JetTorchDataset\n",
    "from source.models.part import AttentionBlock, ParticleTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `yaml` configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "rnd_seed = config['rnd_seed']\n",
    "L.seed_everything(rnd_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hook function for extracting intermediate values of the models\n",
    "\n",
    "See [\"how to extract intermediate values of torch models via hook function\"](https://discuss.pytorch.org/t/how-can-i-extract-intermediate-layer-output-from-loaded-cnn-model/77301) for further detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_model_hook_function(name: str, intermediate_outputs: dict[str, torch.Tensor]):\n",
    "    def hook_function(module, input, output):\n",
    "        intermediate_outputs[name] = output.detach().cpu()\n",
    "    return hook_function\n",
    "\n",
    "def ParT_intermediate_outputs(model: ParticleTransformer) -> dict[str, torch.Tensor]:\n",
    "    intermediate_outputs: dict[str, torch.Tensor] = {}\n",
    "\n",
    "    # 8 particle attention blocks\n",
    "    for i in range(8):\n",
    "        attn_block: AttentionBlock = model.par_attn_blocks[i]\n",
    "        attn_softmax = attn_block.attn.softmax\n",
    "\n",
    "        hook_function = torch_model_hook_function(f\"par_{i}\", intermediate_outputs)\n",
    "        attn_softmax.register_forward_hook(hook_function)\n",
    "\n",
    "    return intermediate_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for load model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_checkpoint(dataset: str, model: str, rnd_seed: int, epoch: int) -> nn.Module:\n",
    "    # Prth to log directory.\n",
    "    log_dir = os.path.join('training_logs', dataset, f\"{model}_{rnd_seed}\", 'lastest_run')\n",
    "\n",
    "    # Load the initial model structure.\n",
    "    model = torch.load(os.path.join(log_dir, 'model.pt'), weights_only=False)\n",
    "\n",
    "    # Path to the model checkpoint.\n",
    "    ckpt_dir = os.path.join(log_dir, 'checkpoints')\n",
    "    ckpt_path = os.path.join(ckpt_dir, f\"epoch={epoch}.ckpt\")\n",
    "\n",
    "    # Checkpoints might have different version due to shutdown during training.\n",
    "    ckpt_version = 1\n",
    "    while os.path.exists(os.path.join(ckpt_dir, f\"epoch={epoch}-v{ckpt_version}.ckpt\")):\n",
    "        ckpt_path = os.path.join(ckpt_dir, f\"epoch={epoch}-v{ckpt_version}.ckpt\")\n",
    "        ckpt_version += 1\n",
    "    ckpt = torch.load(ckpt_path, weights_only=True)\n",
    "\n",
    "    # Load the model.\n",
    "    state_dict = {k.replace('model.', ''): v for k, v in ckpt['state_dict'].items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = globals()[config['dataset']]\n",
    "MODEL = ParticleTransformer\n",
    "\n",
    "# Create the dataset.\n",
    "pad_num_ptcs = config[DATASET.__name__]['pad_num_ptcs']\n",
    "channels = DATASET.CHANNELS\n",
    "jet_events_list = [DATASET(channel=channel) for channel in channels]\n",
    "jet_dataset_list = [JetTorchDataset(jet_events, label, pad_num_ptcs=pad_num_ptcs) for label, jet_events in enumerate(jet_events_list)]\n",
    "fields = jet_dataset_list[0].fields\n",
    "\n",
    "# Number of data points.\n",
    "num_data = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the selected particle features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data\n",
    "particle_features_list = []\n",
    "\n",
    "for jet_events, data_index in itertools.product(jet_events_list, range(num_data)):\n",
    "    \n",
    "    x = jet_events.data[data_index]\n",
    "    \n",
    "    # Select few fields to present in dashboard.\n",
    "    for field in fields:\n",
    "        particle_features_list.append({\n",
    "            'channel': jet_events.channel,\n",
    "            'data_index': data_index,\n",
    "            'feature': field,\n",
    "            'array': ak.to_numpy(x[field])\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the intermediate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs to be stored and loaded in dashboard.\n",
    "intermediate_outputs_list = []\n",
    "\n",
    "# Weights and biases of the first layer.\n",
    "linear_weights = []\n",
    "\n",
    "# Loop over each epoch checkpoint.\n",
    "for epoch_index in range(config['num_epochs']):\n",
    "\n",
    "    # Load the model checkpoint.\n",
    "    model = load_model_checkpoint(\n",
    "        dataset=DATASET.__name__,\n",
    "        model=MODEL.__name__,\n",
    "        rnd_seed=rnd_seed,\n",
    "        epoch=epoch_index,\n",
    "    )\n",
    "\n",
    "    # Store the weights and biases of the first layer.\n",
    "    if MODEL == ParticleTransformer:\n",
    "        w = model.par_embedding.embedding[1].weight.detach().numpy().T\n",
    "        assert len(w.shape) == 2, \"The weight matrix should have 2 dimensions.\"\n",
    "        for i, field in enumerate(fields):\n",
    "            linear_weights.append({\n",
    "                'epoch_index': epoch_index,\n",
    "                'field': field,\n",
    "                'weights': w[i],\n",
    "            })\n",
    "\n",
    "    # Extract intermediate outputs for each channel and data index.\n",
    "    for jet_dataset, data_index in itertools.product(jet_dataset_list, range(num_data)):\n",
    "\n",
    "        x, _ = jet_dataset[data_index]\n",
    "\n",
    "        # Remove the padded particles.\n",
    "        x = x[torch.all(torch.isfinite(x), dim=-1)]\n",
    "\n",
    "        # Reshape for additional the batch dimension.\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        # Fetch intermediate outputs.\n",
    "        if MODEL == ParticleTransformer:\n",
    "            output = ParT_intermediate_outputs(model)\n",
    "            _ = model(x)\n",
    "\n",
    "            # 8 heads in the attention blocks.\n",
    "            for block_index in range(8):\n",
    "                intermediate_outputs_list.append({\n",
    "                    'channel': jet_dataset.channel,\n",
    "                    'data_index': data_index,\n",
    "                    'epoch_index': epoch_index,\n",
    "                    'block_index': block_index,\n",
    "                    'output': output[f\"par_{block_index}\"].squeeze(0)\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store in pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'channels': channels,\n",
    "    'num_data': num_data,\n",
    "    'num_epochs': config['num_epochs'],\n",
    "    'particle_features': pd.DataFrame(particle_features_list),\n",
    "    'intermediate_outputs': pd.DataFrame(intermediate_outputs_list),\n",
    "    'linear_weights': pd.DataFrame(linear_weights),\n",
    "}\n",
    "\n",
    "os.makedirs('intermediate_outputs', exist_ok=True)\n",
    "save_path = os.path.join('intermediate_outputs', f\"{DATASET.__name__}_{MODEL.__name__}_{rnd_seed}.npy\")\n",
    "np.save(save_path, summary, allow_pickle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
