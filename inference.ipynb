{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from source.data import jetclass, jetnet, topqcd\n",
    "from source.data.datamodule import JetLightningDataModule\n",
    "from source.models.litmodel import TorchLightningModule\n",
    "from source.models.part import ParticleTransformer, AttentionBlock\n",
    "from source.models.pnet import ParticleNet\n",
    "\n",
    "dataset = jetclass\n",
    "model_class = ParticleTransformer\n",
    "train_model = False\n",
    "\n",
    "num_data = 10\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hook_fn(name: str, intermediate_outputs: dict[str, torch.Tensor]):\n",
    "    def hook_fn(module, input, output):\n",
    "        intermediate_outputs[name] = output.detach().cpu()\n",
    "    return hook_fn\n",
    "\n",
    "def get_intermediate_outputs(model: ParticleTransformer) -> dict[str, torch.Tensor]:\n",
    "    intermediate_outputs: dict[str, torch.Tensor] = {}\n",
    "\n",
    "    # 8 particle attention blocks\n",
    "    for i in range(8):\n",
    "        attn_block: AttentionBlock = model.par_attn_blocks[i]\n",
    "        attn_softmax = attn_block.attn.softmax\n",
    "\n",
    "        hook_fn = get_hook_fn(f\"par_{i}\", intermediate_outputs)\n",
    "        attn_softmax.register_forward_hook(hook_fn)\n",
    "\n",
    "    return intermediate_outputs\n",
    "\n",
    "def load_ckpt(model: nn.Module, epoch: int) -> nn.Module:\n",
    "    dataset_name: str = dataset.__name__.split('.')[-1]\n",
    "    ckpt_dir = os.path.join('training_logs', dataset_name, model.__class__.__name__, 'lastest_run', 'checkpoints')\n",
    "    ckpt_path = os.path.join(ckpt_dir, f\"epoch={epoch}.ckpt\")\n",
    "    ckpt = torch.load(ckpt_path, weights_only=True)\n",
    "    state_dict = {k.replace('model.', ''): v for k, v in ckpt['state_dict'].items()}\n",
    "    model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating JetLightningDataModule: 100%|██████████| 1/1 [00:02<00:00,  2.63s/it]\n",
      "Creating JetLightningDataModule: 100%|██████████| 1/1 [00:02<00:00,  2.22s/it]\n",
      "Creating JetLightningDataModule: 100%|██████████| 1/1 [00:02<00:00,  2.84s/it]\n",
      "Creating JetLightningDataModule: 100%|██████████| 1/1 [00:01<00:00,  1.83s/it]\n",
      "Creating JetLightningDataModule: 100%|██████████| 1/1 [00:02<00:00,  2.34s/it]\n",
      "Creating JetLightningDataModule: 100%|██████████| 1/1 [00:02<00:00,  2.77s/it]\n",
      "Creating JetLightningDataModule: 100%|██████████| 1/1 [00:02<00:00,  2.09s/it]\n",
      "Creating JetLightningDataModule: 100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n",
      "Creating JetLightningDataModule: 100%|██████████| 1/1 [00:02<00:00,  2.94s/it]\n",
      "Creating JetLightningDataModule: 100%|██████████| 1/1 [00:02<00:00,  2.39s/it]\n"
     ]
    }
   ],
   "source": [
    "# Model setup (`yaml` configuration file)\n",
    "with open(f\"configs/{model_class.__name__}.yaml\", 'r') as file:\n",
    "    hparams = yaml.safe_load(file)[model_class.__name__]\n",
    "    score_dim: int = len(dataset.channels)\n",
    "    model = model_class(score_dim=score_dim, parameters=hparams)\n",
    "\n",
    "particle_features: dict[tuple[str, int], list[tuple[str, np.ndarray]]] = {}\n",
    "intermediate_outputs: dict[tuple[str, int, int, int], np.ndarray] = {}\n",
    "\n",
    "for channel in dataset.channels:\n",
    "    # Create a batch_size == 1 data_module\n",
    "    data_module = JetLightningDataModule(\n",
    "        events_list=[dataset.JetEvents(channel=channel)],\n",
    "        num_train=-1,\n",
    "        num_valid=-1,\n",
    "        num_test=-1,\n",
    "        batch_size=1,\n",
    "    )\n",
    "\n",
    "    for epoch_index in range(num_epochs):\n",
    "        # Load the model from the checkpoint\n",
    "        load_ckpt(model, epoch_index)\n",
    "        model.eval()\n",
    "\n",
    "        # Hook the model to extract intermediate outputs\n",
    "        _intermediate_outputs = get_intermediate_outputs(model)\n",
    "\n",
    "        for data_index in range(num_data):\n",
    "            # Load the data with index == data_index\n",
    "            x, y_true = data_module.data_list[0][data_index]\n",
    "\n",
    "            # Remove padded particles\n",
    "            x = x[torch.all(torch.isfinite(x), dim=-1)]\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "            # Save the features\n",
    "            if epoch_index == 0:\n",
    "                fields = data_module.fields\n",
    "                selected_fields = [\n",
    "                    'log_part_pt',\n",
    "                    'part_charge',\n",
    "                    'part_dR',\n",
    "                ]\n",
    "                _particle_features = []\n",
    "                for i, field in enumerate(fields):\n",
    "                    if field in selected_fields:\n",
    "                        _feature = x[0, :, i].detach().cpu().numpy().reshape(-1)\n",
    "                        _min_value = np.min(_feature)\n",
    "                        _max_value = np.max(_feature)\n",
    "                        _pad_value = 2 * _min_value - _max_value\n",
    "                        _diagonal = np.full((len(_feature), len(_feature)), _pad_value)\n",
    "                        np.fill_diagonal(_diagonal, _feature)\n",
    "                        _particle_features.append((field, _diagonal))\n",
    "                particle_features[(channel, data_index)] = _particle_features\n",
    "\n",
    "            y_pred = model(x)\n",
    "\n",
    "            for block_index in range(8):\n",
    "                intermediate_outputs[(channel, data_index, epoch_index, block_index)] = _intermediate_outputs[f\"par_{block_index}\"].squeeze(0)\n",
    "\n",
    "summary = {\n",
    "    'num_data': num_data,\n",
    "    'num_epochs': num_epochs,\n",
    "    'particle_features': particle_features,\n",
    "    'intermediate_outputs': intermediate_outputs,\n",
    "}\n",
    "\n",
    "np.save(f\"{dataset.__name__.split('.')[-1]}.npy\", summary, allow_pickle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
